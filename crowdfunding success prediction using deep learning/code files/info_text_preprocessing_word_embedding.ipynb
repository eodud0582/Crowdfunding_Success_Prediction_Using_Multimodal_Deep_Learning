{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 전처리 및 분석 and 언어 모델 및 워드 임베딩 개념/설명\n",
    "- 클렌징(cleansing)\n",
    "- 불용어(stopwords) 제거\n",
    "- 토큰화(tokenize)\n",
    "- 형태소 분석 : 어근추출(Stemming), 원형복원(Lemmatization), 품사부착(POS Tagging))\n",
    "- 워드 임베딩(Word Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame을 자동 분석 및 리포트 생성\n",
    "import pandas_profiling\n",
    "pr = dataframe.profile_report()\n",
    "#dataframe.profile_report() 또는 pr -- 바로 리포트 결과 보기\n",
    "pr.to_file('./pr_report.html') #리포트를 html 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화(Tokenization)\n",
    "\n",
    "# 단어 토큰화(Word Tokenization)\n",
    "# 1) 구두점이나 특수 문자를 단순 제외해서는 안 된다\n",
    "# 2) 줄임말과 단어 내에 띄어쓰기가 있는 경우\n",
    "# 문장 토큰화(Sentence Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tb_tokenizer = TreebankWordTokenizer()\n",
    "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "print(tb_tokenizer.tokenize(text))\n",
    "\n",
    "# 표준으로 쓰이고 있는 토큰화 방법 중 하나인 Penn Treebank Tokenization의 규칙\n",
    "# 규칙 1. 하이푼으로 구성된 단어는 하나로 유지\n",
    "# 규칙 2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리\n",
    "# 규칙1과 규칙2에 따라서 home-based는 하나의 토큰으로 취급; dosen't의 경우 does와 n't는 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  \n",
    "print(word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))  \n",
    "# word_tokenize는 Don't를 Do와 n't로 분리하였으며, 반면 Jone's는 Jone과 's로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal', '.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(\"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer  \n",
    "print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n",
    "# NLTK의 WordPunctTokenizer는 구두점을 별도로 분류하는 특징을 갖고 있기때문에, word_tokenize와는 달리 Don't를 Don과 '와 t로 분리\n",
    "# 이와 마찬가지로 Jone's를 Jone과 '와 s로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home', '-', 'based', 'restaurant', 'may', 'be', 'an', 'ideal', '.', 'it', 'doesn', \"'\", 't', 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "print(WordPunctTokenizer().tokenize(\"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "print(text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))\n",
    "# Keras의 text_to_word_sequence는 기본적으로 모든 알파벳을 소문자로 바꾸면서 온점이나 컴마, 느낌표 등의 구두점을 제거\n",
    "# 하지만 don't나 jone's와 같은 경우 아포스트로피는 보존"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['starting', 'a', 'home', 'based', 'restaurant', 'may', 'be', 'an', 'ideal', 'it', \"doesn't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own']\n"
     ]
    }
   ],
   "source": [
    "print(text_to_word_sequence(\"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정제(Cleaning) and 정규화(Normalization)\n",
    "\n",
    "# 1. 규칙에 기반한 표기가 다른 단어들의 통합\n",
    "# 2. 대, 소문자 통합\n",
    "# 3. 불필요한 단어의 제거(Removing Unnecessary Words)\n",
    "# (1) 등장 빈도가 적은 단어(Removing Rare words)\n",
    "# (2) 길이가 짧은 단어(Removing words with very a short length)\n",
    "# 4. 정규 표현식(Regular Expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "# 영어권 언어에서는 길이가 짧은 단어를 삭제하는 것만으로도 어느정도 자연어 처리에서 크게 의미가 없는 단어들을 제거하는 효과를 볼 수 있음\n",
    "# 즉, 영어권 언어에서 길이가 짧은 단어들은 대부분 불용어에 해당\n",
    "# 길이가 짧은 단어를 제거하는 2차 이유는 길이를 조건으로 텍스트를 삭제하면서 단어가 아닌 구두점들까지도 한꺼번에 제거하기 위함도 있음\n",
    "\n",
    "import re\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "print(shortword.sub('', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩(Integer Encoding)\n",
    "# 보통은 전처리 또는 빈도수가 높은 단어들만 사용하기 위해서 단어에 대한 빈도수를 기준으로 정렬한 뒤에 인덱스 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
      "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n"
     ]
    }
   ],
   "source": [
    "# 정제와 단어 토큰화\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text=\"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\n",
    "text=sent_tokenize(text)\n",
    "\n",
    "vocab={} # 단어 빈도수를 담을 dictionary\n",
    "sentences = [] # 토큰화된 단어를 담을 \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in text:\n",
    "    sentence=word_tokenize(i) # 단어 토큰화를 수행합니다.\n",
    "    result = []\n",
    "\n",
    "    for word in sentence: \n",
    "        word=word.lower() # 모든 단어를 소문자화하여 단어의 개수 줄이기\n",
    "        if word not in stop_words: # 단어 토큰화 된 결과에 대해서 불용어 제거\n",
    "            if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어 제거\n",
    "                result.append(word)\n",
    "                if word not in vocab: #단어가 새로운 단어라면\n",
    "                    vocab[word] = 0 #0부터 count\n",
    "                vocab[word] += 1 #이미 있는 단어라면 count 1 추가\n",
    "    sentences.append(result) # 각 문장별로 토큰화 된 단어들의 리스트를 이 리스트에 모은다\n",
    "print(sentences)\n",
    "print(vocab) # 단어별 빈도수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 빈도수가 높은 순서대로 정렬\n",
    "vocab_sorted = sorted(vocab.items(), key=lambda x:x[1], reverse=True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n",
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counter\n",
    "from collections import Counter\n",
    "words = sum(sentences, []) # 각 문장별로 리스트로 묶여있던 토큰들을 다 풀어서 합쳐서 하나의 리스트로; words = np.hstack(sentences)\n",
    "print(words)\n",
    "vocab = Counter(words) #단어 빈도수 계산 in 키(key):값(value) 형태\n",
    "print(vocab)\n",
    "\n",
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) #등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK의 FreqDist\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "vocab = FreqDist(np.hstack(sentences)) # np.hstack으로 문장 구분을 제거하여 입력으로 사용 (예: ['barber', 'person', 'barber', 'good', ...])\n",
    "\n",
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) #등장 빈도수가 높은 상위 5개의 단어만 저장\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "# 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스 부여\n",
    "vocab = [('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n",
    "word_to_index = {word[0] : index+1 for index, word in enumerate(vocab)}\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 4), ('word', 2), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n",
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "# Keras의 텍스트 전처리\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "sentences=[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
    "keras_tokenizer = Tokenizer()\n",
    "keras_tokenizer.fit_on_texts(sentences) ## fit_on_texts()안에 코퍼스(리스트)를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성\n",
    "print(keras_tokenizer.word_index) #각 단어 인덱스; 단어 빈도수가 높은 순으로 낮은 정수 인덱스를 부여\n",
    "print(keras_tokenizer.word_counts) # 각 단어 카운트\n",
    "print(keras_tokenizer.texts_to_sequences(sentences)) #입력으로 들어온 코퍼스에 대해서 각 단어를 이미 정해진 인덱스로 변환\n",
    "\n",
    "# 상위 5개 단어만 사용하고자 한다면\n",
    "vocab_size = 5\n",
    "keras_tokenizer = Tokenizer(num_words=vocab_size+1)\n",
    "keras_tokenizer.fit_on_texts(sentences)\n",
    "# 이 다음은 바로 위에서 print했던 것들 똑같이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원-핫 인코딩(One-hot Encoding)\n",
    "# 단어의 개수가 늘어날 수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점\n",
    "# 단어 간 유사성을 알 수 없다는 단점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n",
      "[2, 5, 1, 6, 3, 7]\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# 케라스(Keras)를 이용한 원-핫 인코딩(One-hot encoding)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "text=\"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
    "\n",
    "k_t = Tokenizer()\n",
    "k_t.fit_on_texts([text]) #리스트\n",
    "print(k_t.word_index) #각 단어에 대한 인코딩 결과 출력\n",
    "\n",
    "sub_text=\"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
    "encoded = k_t.texts_to_sequences([sub_text])[0]\n",
    "print(encoded)\n",
    "\n",
    "one_hot = to_categorical(encoded)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원-핫 인코딩의 단점을 해결하기 위해 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화 하는 기법으로 크게 두 가지가 있음\n",
    "# 첫째는 카운트 기반의 벡터화 방법인 LSA, HAL 등이 있으며, 둘째는 예측 기반으로 벡터화하는 NNLM, RNNLM, Word2Vec, FastText 등이 있다\n",
    "# 그리고 카운트 기반과 예측 기반 두 가지 방법을 모두 사용하는 방법으로 GloVe라는 방법이 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 분리하기\n",
    "# 단어 분리(Subword segmenation) 작업은 하나의 단어는 의미있는 여러 내부 단어들(subwords)의 조합으로 구성된 경우가 많기 때문에,\n",
    "# 단어를 여러 단어로 분리해서 단어를 이해해보겠다는 의도를 가진 전처리 작업 --> 단어 분리 토크나이저\n",
    "# Byte Pair Encoding (BPE) -- 실무에 사용하기에는 속도가 매우 느리다\n",
    "# 센텐스피스(Sentencepiece) -- BPE보다 빠르며, 사전 토큰화 작업없이 단어 분리 토큰화를 수행하므로 언어에 종속되지 않는다\n",
    "    # https://github.com/google/sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언어 모델\n",
    "# 크게 국소 표현(Local Representation) 방법과 분산 표현(Distributed Representation) 방법으로 나뉜다\n",
    "    # 국소 표현 = 이산 표현(Discrete Representation)\n",
    "    # 분산 표현 = 연속 표현(Continuous Represnetation) -- 연속표현이 분산표현을 포괄하는 더 큰 개념이라는 설명도 있음\n",
    "# 국소 표현 방법은 해당 단어 그 자체만 보고, 특정값을 맵핑하여 단어를 표현하는 방법\n",
    "# 분산 표현 방법은 그 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법; 단어의 뉘앙스를 반영\n",
    "# 국소 표현 종류\n",
    "    # One-hot Vector, N-gram\n",
    "    # Count based : Bag of Words\n",
    "        # DTM, TF-IDF\n",
    "# 연속 표현 종류\n",
    "    # Prediction based : Word2Vec, FestText\n",
    "    # Count based : LSA\n",
    "    # Prediction + Count based : Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 워드 임베딩(Word Embedding)\n",
    "# 단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법\n",
    "# 밀집 표현(Dense Representation)/밀집 벡터 : 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞추고, 이 과정에서 더 이상 0과 1만 가진 값이 아니라 실수값을 가지게 됨\n",
    "# 반대) 희소 표현(Sparse Representation)/희소 벡터 : 벡터 또는 행렬(matrix)의 값이 1과 0, 대부분이 0으로 표현되는 방법 (예: 원-핫 벡터)\n",
    "    # 벡터의 차원이 단어 집합(vocabulary)의 크기; 공간적 낭비; 단어 의미/유사도 X\n",
    "# 이 밀집 벡터를 워드 임베딩 과정을 통해 나온 결과라고 하여 임베딩 벡터(embedding vector)라고도 함\n",
    "# 분산 표현을 이용하여 단어의 유사도를 벡터화하는 작업은 워드 임베딩에 속함\n",
    "# 워드 임베딩 방법론으로는 LSA, Word2Vec, FastText, Glove 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "# 희소 표현이 고차원에 각 차원이 분리된 표현 방법이었다면, 분산 표현은 저차원에 단어의 의미를 여러 차원에다가 분산하여 표현; 이렇게 단어 간 유사도를 계산할 수 있음\n",
    "# 이를 위한 학습 방법으로는 NNLM, RNNLM 등이 있으나 요즘에는 해당 방법들의 속도를 대폭 개선시킨 Word2Vec가 많이 쓰임\n",
    "# 두 가지 방식\n",
    "    # CBOW(Continuous Bag of Words): 주변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측\n",
    "    # Skip-Gram: 중간에 있는 단어로 주변 단어들을 예측하는 방법\n",
    "# 여러 논문에서 성능 비교를 진행했을 때, 전반적으로 Skip-gram이 CBOW보다 성능이 좋다고 알려져 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네거티브 샘플링(Negative Sampling)\n",
    "# 대체적으로 Word2Vec를 사용한다고 하면 SGNS(Skip-Gram with Negative Sampling)을 사용\n",
    "# 전체 단어 집합보다 훨씬 작은 단어 집합을 만들어놓고 마지막 단계를 이진 분류 문제로 바꿔버리는 것\n",
    "# 주변 단어들을 긍정(positive)으로 두고 랜덤으로 샘플링 된 단어들을 부정(negative)으로 둔 다음에 이진 분류 문제를 수행\n",
    "# 이는 기존의 다중 클래스 분류 문제를 이진 분류 문제로 바꾸면서도 연산량에 있어서 훨씬 효율적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
