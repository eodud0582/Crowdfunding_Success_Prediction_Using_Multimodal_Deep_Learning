{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jDl3p4d-DcB4"
   },
   "source": [
    "# Kickstarter 사이트의 펀딩 페이지의 content 및 risk and challenges 텍스트 마이닝\n",
    "- 2015년~2019년 데이터만 수집\n",
    "- 총 수집된 데이터 수/행 : 153,576 (608.5MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eNMdmkGZDcD2"
   },
   "source": [
    "- 2015~2019년 각 연도별로 나누어 텍스트 데이터 수집 진행\n",
    "- Kickstarter의 펀딩 페이지는 javascript를 통해 동적으로 텍스트 내용이 불러와지기에 Requests가 아닌 Selenium 사용\n",
    "- 웹크롤링으로 데이터 수집 중 지속적으로 차단을 당하여 여러번의 수정을 거쳤다\n",
    "    1. 오류가 발생하면 크롤링이 중단되고 그 중단 지점(index)이 csv에 저장되어 그 지점부터 manually 다시 크롤링을 시작\n",
    "    2. 오류 발생시 크롤링을 중단하지 않고 계속 진행하되 오류가 난 지점들을 csv에 저장하고, 각 연도에 대한 크롤링이 완료되면 한꺼번에 오류가 난 부분들을 다시 크롤링\n",
    "    3. 크롤링이 차단 당하거나 오류가 날시, continue 하여 다음 url로 바로 넘어가는게 아니라 n분 간격으로 n번까지 더 시도해서 scrape해보고 그럼에도 계속 오류가 발생하면 그 지점은 나중에 한꺼번에 재크롤링 할 수 있게 저장하고, 다음 url로 넘어가 이어서 scrape하도록 설계\n",
    "    4. 최종적으로는, 로봇처럼 보이지 않도록, 위 3번 방법에 랜덤한 time.sleep을 추가하고 다양한 범위 시도 끝에 많이 느리지 않으면서도 차단을 당하지 않는 최적의 알고리즘 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTMtSNZ5DcEB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver import ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "options = ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.117 Safari/537.36\")\n",
    "#path = '/Users/Python/chromedriver'\n",
    "#driver = Chrome(executable_path=path, options=options)\n",
    "driver = Chrome(options=options)\n",
    "\n",
    "filename_contents = 'edited_content_2019.csv'\n",
    "filename_errors = 'new_errors_2019.csv'\n",
    "#content_list = []\n",
    "\n",
    "for idx, url in enumerate(url_list):\n",
    "    driver.get(url)\n",
    "    time.sleep(random.uniform(8,20))\n",
    "    req = driver.page_source\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "    try:\n",
    "        # content 텍스트 데이터 수집\n",
    "        content_tag = soup.select_one('div.rte__content')\n",
    "        contents = content_tag.select('p')\n",
    "        contents_collected = []\n",
    "        for c in contents:\n",
    "            content = c.get_text().strip()\n",
    "            contents_collected.append(content) #한 프로젝트의 페이지 내 모든 설명/내용들을 하나로 모으고\n",
    "        # risk and challenge 텍스트 데이터 수집\n",
    "        try:\n",
    "            risk_challenge_tag = soup.select_one('div#risksAndChallenges')\n",
    "            risk_challenge_list = risk_challenge_tag.select('p.js-risks-text.text-preline')\n",
    "            for rc in risk_challenge_list:\n",
    "                risk_challenge = rc.get_text().strip()\n",
    "        except:\n",
    "            risk_challenge=\" \"\n",
    "        \n",
    "        # content_list.append([idx, contents_collected, risk_challenge]) # 크롤링 된 펀딩 내용(텍스트)도 변수에 저장 \n",
    "        \n",
    "        #오류 지점 이후 이어서 크롤링 할 때, index 순서 관계없이 일단 csv파일에 바로 이어서 한줄씩 append (이후 최종 취합 후 index 번호로 정렬)\n",
    "        content_list_since_error = []\n",
    "        content_list_since_error.append([idx, contents_collected, risk_challenge])\n",
    "        pd.DataFrame(content_list_since_error, columns=['index','content','risk_challenge']).to_csv(filename_contents, index=False, header=False, mode='a', encoding='UTF-8')\n",
    "        print(idx) #한 url에 대한 크롤링이 끝날 때마다 해당 index번호 출력\n",
    "    \n",
    "    # 오류 발생시 10번 재시도 \n",
    "    except Exception as ex:\n",
    "        print(\"Unexpected error at {}\".format(idx), ex) #오류 지점 출력\n",
    "        error_url = url\n",
    "        count = 0\n",
    "        while count < 11:\n",
    "            print(\"Retrying:\", count)\n",
    "            driver.get(error_url)\n",
    "            time.sleep(random.uniform(8,20))\n",
    "            req = driver.page_source\n",
    "            soup = BeautifulSoup(req, 'lxml')\n",
    "            try:\n",
    "                content_tag = soup.select_one('div.rte__content')\n",
    "                contents = content_tag.select('p')\n",
    "                contents_collected = []\n",
    "                for c in contents:\n",
    "                    content = c.get_text().strip()\n",
    "                    contents_collected.append(content)\n",
    "                \n",
    "                try:\n",
    "                    risk_challenge_tag = soup.select_one('div#risksAndChallenges')\n",
    "                    risk_challenge_list = risk_challenge_tag.select('p.js-risks-text.text-preline')\n",
    "                    for rc in risk_challenge_list:\n",
    "                        risk_challenge = rc.get_text().strip()\n",
    "                except:\n",
    "                    risk_challenge=\" \"\n",
    "                \n",
    "                #오류 건에 대한 크롤링 재시도에서 크롤링 성공한 텍스트 데이터 csv에 추가/저장\n",
    "                content_list_since_error = [] \n",
    "                content_list_since_error.append([idx, contents_collected, risk_challenge])\n",
    "                pd.DataFrame(content_list_since_error, columns=['index','content','risk_challenge']).to_csv(filename_contents, index=False, header=False, mode='a', encoding='UTF-8')\n",
    "                print(idx)\n",
    "                break\n",
    "                \n",
    "            except Exception as et:\n",
    "                print(\"Error while retrying:\", et)\n",
    "                time.sleep(random.uniform(100,130))\n",
    "                count += 1\n",
    "                continue\n",
    "        \n",
    "        #만약 재시도를 10번 했는데도 크롤링에 실패한다면, 원래 했던대로 오류 index, url, nan값을 csv파일들에 저장\n",
    "        if count == 11:\n",
    "            # content_list.append([idx, error_url, np.nan])\n",
    "            \n",
    "            # 오류 index, url 별도 오류 csv에 저장\n",
    "            error_list = []\n",
    "            error_list.append([idx, error_url])\n",
    "            pd.DataFrame(error_list, columns=['index','error_url']).to_csv(filename_errors, index=False, header=False, mode='a', encoding='UTF-8')\n",
    "            \n",
    "            #오류난 지점에 대한 index, url도 일단은 텍스트 데이터 쌓고있는 csv 파일에 저장 (이후 재크롤링 후 대체)\n",
    "            error_into_saved_csv = []\n",
    "            error_into_saved_csv.append([idx, error_url, np.nan]) # risk_challenge 컬럼에 대한 값은 null로 지정\n",
    "            pd.DataFrame(error_into_saved_csv, columns=['index','content','risk_challenge']).to_csv(filename_contents, index=False, header=False, mode='a', encoding='UTF-8')\n",
    "        \n",
    "        continue\n",
    "    \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ku9a_h4gDcEE"
   },
   "outputs": [],
   "source": [
    "# Google Colab에서 Selenium 사용\n",
    "# 재시도 횟수 --> 4번으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mcfxj1BIEMUL"
   },
   "outputs": [],
   "source": [
    "!apt update\n",
    "!apt install chromium-chromedriver\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WKenEeHlDcEI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver import ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "driver = Chrome('chromedriver', options=options)\n",
    "\n",
    "#저장할 csv파일 이름/경로 지정 (Google Drive)\n",
    "filename_contents = '/content/drive/My Drive/PlayData 빅데이터 분석 전문가 과정/Final_Project/Kickstarter_2019_12_12/content_crawled_2016.csv'\n",
    "filename_errors = '/content/drive/My Drive/PlayData 빅데이터 분석 전문가 과정/Final_Project/Kickstarter_2019_12_12/errors_crawled_2016.csv'\n",
    "\n",
    "for idx, url in enumerate(url_ks_2016[2165:], start=2165):\n",
    "    driver.get(url)\n",
    "    time.sleep(random.uniform(8,20))\n",
    "    req = driver.page_source\n",
    "    soup = BeautifulSoup(req, 'lxml')\n",
    "\n",
    "    try:\n",
    "        content_tag = soup.select_one('div.rte__content')\n",
    "        contents = content_tag.select('p')\n",
    "        contents_collected = []\n",
    "        for c in contents:\n",
    "            content = c.get_text().strip()\n",
    "            contents_collected.append(content)\n",
    "\n",
    "        try:\n",
    "            risk_challenge_tag = soup.select_one('div#risksAndChallenges')\n",
    "            risk_challenge_list = risk_challenge_tag.select('p.js-risks-text.text-preline')\n",
    "            for rc in risk_challenge_list:\n",
    "                risk_challenge = rc.get_text().strip()\n",
    "        except:\n",
    "            risk_challenge=\" \"\n",
    "        \n",
    "        content_list_since_error = []\n",
    "        content_list_since_error.append([idx, contents_collected, risk_challenge])\n",
    "        pd.DataFrame(content_list_since_error, columns=['index','content','risk_challenge']).to_csv(filename_contents, index=False, header=False, mode='a', encoding='UTF-8')\n",
    "        print(idx)\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(\"Unexpected error at {}\".format(idx), ex)\n",
    "        error_url = url\n",
    "        count = 0\n",
    "        while count < 5:\n",
    "            print(\"Retrying:\", count)\n",
    "            driver.get(error_url)\n",
    "            time.sleep(random.uniform(8,20))\n",
    "            req = driver.page_source\n",
    "            soup = BeautifulSoup(req, 'lxml')\n",
    "\n",
    "            try:\n",
    "                content_tag = soup.select_one('div.rte__content')\n",
    "                contents = content_tag.select('p')\n",
    "                contents_collected = []\n",
    "                for c in contents:\n",
    "                    content = c.get_text().strip()\n",
    "                    contents_collected.append(content)\n",
    "\n",
    "                try:\n",
    "                    risk_challenge_tag = soup.select_one('div#risksAndChallenges')\n",
    "                    risk_challenge_list = risk_challenge_tag.select('p.js-risks-text.text-preline')\n",
    "                    for rc in risk_challenge_list:\n",
    "                        risk_challenge = rc.get_text().strip()\n",
    "                except:\n",
    "                    risk_challenge=\" \"\n",
    "\n",
    "                content_list_since_error = []\n",
    "                content_list_since_error.append([idx, contents_collected, risk_challenge])\n",
    "                pd.DataFrame(content_list_since_error, columns=['index','content','risk_challenge']).to_csv(filename_contents, index=False, header=False, mode='a', encoding='UTF-8')\n",
    "                print(\"Retry successful\")\n",
    "                print(idx)\n",
    "                break\n",
    "\n",
    "            except Exception as et:\n",
    "                print(\"Error while retrying:\", et)\n",
    "                time.sleep(random.uniform(100,130))\n",
    "                count += 1\n",
    "                continue\n",
    "\n",
    "        if count == 5:\n",
    "            error_into_saved_csv = []\n",
    "            error_into_saved_csv.append([idx, np.nan, np.nan])\n",
    "            pd.DataFrame(error_into_saved_csv, columns=['index','content','risk_challenge']).to_csv(filename_contents, index=False, header=False, mode='a', encoding='UTF-8')\n",
    "            \n",
    "            error_list = []\n",
    "            error_list.append([idx, error_url])\n",
    "            pd.DataFrame(error_list, columns=['index','error_url']).to_csv(filename_errors, index=False, header=False, mode='a', encoding='UTF-8')\n",
    "            print(\"Retry failed\")\n",
    "\n",
    "        continue\n",
    "\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "general_web_crawling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
